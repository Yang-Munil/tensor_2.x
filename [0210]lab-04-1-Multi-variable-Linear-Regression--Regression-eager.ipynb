{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa55_des11wj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 04 - Multi-variable Linear Regression\n",
    "\n",
    "<img width=\"200\" src=\"https://i.imgur.com/hbPVe1T.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1622,
     "status": "ok",
     "timestamp": 1549861753492,
     "user": {
      "displayName": "Sean-June Lee",
      "photoUrl": "https://lh3.googleusercontent.com/-j96Pvw9LbEw/AAAAAAAAAAI/AAAAAAACQxw/aY8i4PoItwc/s64/photo.jpg",
      "userId": "12576390002043678893"
     },
     "user_tz": -540
    },
    "id": "BbOQ25cb11wl",
    "outputId": "182e949f-9183-4095-84bb-3edcf3f768fa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXdPAm1y11xP",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Multi-variable linear regression\n",
    "Predicting exam score - regression using three inputs (x1, x2, x3)\n",
    "\n",
    "x1 (quiz 1) | x2 (quiz 2) | x3 (mid 1) | Y (final)\n",
    "---- | ---- | ----| ----\n",
    "73 | 80 | 75 | 152\n",
    "93 | 88 | 93 | 185\n",
    "89 | 91 | 90 | 180\n",
    "96 | 98 | 100 | 196\n",
    "73 | 66 | 70 | 142\n",
    "\n",
    "\n",
    "Test Scores for General Psychology ( https://goo.gl/g2T8Kp )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCc0H5qT11xU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matrix multiplication\n",
    "\n",
    "## dot product(=scalar product, 내적)\n",
    "<img src=\"https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg\" >\n",
    "\n",
    "\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOpVHPfX11xV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-feature regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "$$ H(x) = w x + b $$\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jiJiS73i11xW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = \\underline{w_1 x_1 + w_2 x_2 + w_3 x_3} + b $$\n",
    "\n",
    "$$ w_1 x_1 + w_2 x_2 + w_3 x_3 $$ \n",
    "\n",
    "$$ \\begin{pmatrix} w_{ 1 } & w_{ 2 } & w_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} x_{ 1 } \\\\ x_{ 2 } \\\\ x_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ WX $$ (W, X 는 matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qyo_v2q211xX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis without b\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$$\n",
    "\n",
    "$$ = b + w_1 x_1 + w_2 x_2 + w_3 x_3 $$\n",
    "\n",
    "$$ = \\begin{pmatrix} b & x_{ 1 } & x_{ 2 } & x_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} 1 \\\\ w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ = XW $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70JryWbi11xZ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix \n",
    "\n",
    "### Many x instances\n",
    "\n",
    "$$ \\begin{pmatrix} x_{ 11 } & x_{ 12 } & x_{ 13 } \\\\ x_{ 21 } & x_{ 22 } & x_{ 23 } \\\\ x_{ 31 } & x_{ 32 } & x_{ 33 }\\\\ x_{ 41 } & x_{ 42 } & x_{ 43 }\\\\ x_{ 51 } & x_{ 52 } & x_{ 53 }\\end{pmatrix} \\cdot \\begin{pmatrix} w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix}=\\begin{pmatrix} x_{ 11 }w_{ 1 }+x_{ 12 }w_{ 2 }+x_{ 13 }w_{ 3 } \\\\ x_{ 21 }w_{ 1 }+x_{ 22 }w_{ 2 }+x_{ 23 }w_{ 3 }\\\\ x_{ 31 }w_{ 1 }+x_{ 32 }w_{ 2 }+x_{ 33 }w_{ 3 } \\\\ x_{ 41 }w_{ 1 }+x_{ 42 }w_{ 2 }+x_{ 43 }w_{ 3 } \\\\ x_{ 51 }w_{ 1 }+x_{ 52 }w_{ 2 }+x_{ 53 }w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ [5, 3] \\cdot [3, 1] = [5, 1] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "5는 데이터(instance)의 수, 3은 변수(feature)의 수, 1은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PmX23KO11xc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis using matrix (n output)\n",
    "\n",
    "$$ [n, 3] \\cdot [?, ?] = [n, 2] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "* n은 데이터(instance)의 개수, 2는 결과 값의 개수로 주어진다.\n",
    "* 이때, W [?, ?] ⇒ [3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TdLqWOL811xd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WX vs XW\n",
    "\n",
    "### Theory (Lecture) :\n",
    " $$ H(x) = Wx + b  $$\n",
    "\n",
    "### TensorFlow (Implementation) :\n",
    "\n",
    "$$ H(X) = XW $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hzOpk9w11xf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Example (2 variables)\n",
    "\n",
    "x1 | x2 | y\n",
    "---- | ---- | ----\n",
    "1  |  0  |  1\n",
    "0  |  2  |  2\n",
    "3  |  0  |  3\n",
    "0  |  4  |  4\n",
    "5  |  0  |  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b645aPucdmMz"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2134,
     "status": "ok",
     "timestamp": 1549859056786,
     "user": {
      "displayName": "Sean-June Lee",
      "photoUrl": "https://lh3.googleusercontent.com/-j96Pvw9LbEw/AAAAAAAAAAI/AAAAAAACQxw/aY8i4PoItwc/s64/photo.jpg",
      "userId": "12576390002043678893"
     },
     "user_tz": -540
    },
    "id": "XdIC0_tqpmc1",
    "outputId": "441c62e7-dac2-48ee-affe-fcd6e346cce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 335.280823 |    -4.0663 |     1.1220 |  -6.065215\n",
      "   50 |  76.037262 |    -0.8001 |     1.6209 |  -4.978779\n",
      "  100 |  18.959263 |     0.7151 |     1.8781 |  -4.429109\n",
      "  150 |   6.310240 |     1.4125 |     2.0104 |  -4.134423\n",
      "  200 |   3.445082 |     1.7284 |     2.0768 |  -3.961648\n",
      "  250 |   2.743659 |     1.8667 |     2.1075 |  -3.847750\n",
      "  300 |   2.525401 |     1.9225 |     2.1184 |  -3.762738\n",
      "  350 |   2.417754 |     1.9402 |     2.1181 |  -3.692262\n",
      "  400 |   2.337300 |     1.9403 |     2.1114 |  -3.629400\n",
      "  450 |   2.264998 |     1.9325 |     2.1008 |  -3.570778\n",
      "  500 |   2.196328 |     1.9213 |     2.0881 |  -3.514729\n",
      "  550 |   2.130126 |     1.9085 |     2.0741 |  -3.460409\n",
      "  600 |   2.066037 |     1.8953 |     2.0595 |  -3.407385\n",
      "  650 |   2.003917 |     1.8819 |     2.0444 |  -3.355424\n",
      "  700 |   1.943679 |     1.8686 |     2.0293 |  -3.304398\n",
      "  750 |   1.885258 |     1.8555 |     2.0141 |  -3.254230\n",
      "  800 |   1.828595 |     1.8425 |     1.9990 |  -3.204873\n",
      "  850 |   1.773636 |     1.8297 |     1.9841 |  -3.156293\n",
      "  900 |   1.720329 |     1.8171 |     1.9693 |  -3.108468\n",
      "  950 |   1.668625 |     1.8048 |     1.9547 |  -3.061379\n",
      " 1000 |   1.618474 |     1.7926 |     1.9403 |  -3.015011\n"
     ]
    }
   ],
   "source": [
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "b  = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNHVQS-311xk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Example (2 variables with Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3709,
     "status": "ok",
     "timestamp": 1549859058376,
     "user": {
      "displayName": "Sean-June Lee",
      "photoUrl": "https://lh3.googleusercontent.com/-j96Pvw9LbEw/AAAAAAAAAAI/AAAAAAACQxw/aY8i4PoItwc/s64/photo.jpg",
      "userId": "12576390002043678893"
     },
     "user_tz": -540
    },
    "id": "1SWH2f6vnjCO",
    "outputId": "e37b1216-8540-4aa1-ea9e-3a52761230c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  36.403778 |    -0.6231 |    -0.3508 |  -0.961774\n",
      "   50 |   9.372901 |     0.2914 |     0.1682 |  -0.557764\n",
      "  100 |   2.639858 |     0.7060 |     0.4867 |  -0.347756\n",
      "  150 |   0.825069 |     0.8912 |     0.6846 |  -0.235665\n",
      "  200 |   0.284990 |     0.9721 |     0.8088 |  -0.174012\n",
      "  250 |   0.106844 |     1.0062 |     0.8873 |  -0.138953\n",
      "  300 |   0.042677 |     1.0195 |     0.9372 |  -0.118279\n",
      "  350 |   0.018044 |     1.0241 |     0.9690 |  -0.105598\n",
      "  400 |   0.008188 |     1.0250 |     0.9893 |  -0.097477\n",
      "  450 |   0.004138 |     1.0246 |     1.0022 |  -0.092026\n",
      "  500 |   0.002439 |     1.0239 |     1.0104 |  -0.088173\n",
      "  550 |   0.001710 |     1.0230 |     1.0156 |  -0.085299\n",
      "  600 |   0.001384 |     1.0223 |     1.0188 |  -0.083036\n",
      "  650 |   0.001227 |     1.0217 |     1.0207 |  -0.081161\n",
      "  700 |   0.001142 |     1.0212 |     1.0218 |  -0.079538\n",
      "  750 |   0.001088 |     1.0207 |     1.0224 |  -0.078080\n",
      "  800 |   0.001046 |     1.0203 |     1.0227 |  -0.076735\n",
      "  850 |   0.001011 |     1.0199 |     1.0227 |  -0.075468\n",
      "  900 |   0.000980 |     1.0196 |     1.0226 |  -0.074258\n",
      "  950 |   0.000949 |     1.0192 |     1.0225 |  -0.073089\n",
      " 1000 |   0.000921 |     1.0189 |     1.0222 |  -0.071954\n"
     ]
    }
   ],
   "source": [
    "x_data = [\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUs7qbTR11xs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis without b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3701,
     "status": "ok",
     "timestamp": 1549859058377,
     "user": {
      "displayName": "Sean-June Lee",
      "photoUrl": "https://lh3.googleusercontent.com/-j96Pvw9LbEw/AAAAAAAAAAI/AAAAAAACQxw/aY8i4PoItwc/s64/photo.jpg",
      "userId": "12576390002043678893"
     },
     "user_tz": -540
    },
    "id": "114nIzJq5EuC",
    "outputId": "7b4a0292-0128-4a94-dfac-725f1311ecae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  16.019751 |    -0.1985 |     0.3424 |    -0.6835\n",
      "   50 |   5.635924 |     0.0582 |     0.6809 |    -0.1215\n",
      "  100 |   2.141112 |     0.1997 |     0.8238 |     0.2356\n",
      "  150 |   0.862825 |     0.2786 |     0.8808 |     0.4641\n",
      "  200 |   0.367090 |     0.3227 |     0.9015 |     0.6112\n",
      "  250 |   0.167513 |     0.3468 |     0.9074 |     0.7064\n",
      "  300 |   0.085210 |     0.3593 |     0.9082 |     0.7684\n",
      "  350 |   0.050615 |     0.3649 |     0.9074 |     0.8090\n",
      "  400 |   0.035731 |     0.3663 |     0.9067 |     0.8359\n",
      "  450 |   0.029064 |     0.3651 |     0.9063 |     0.8539\n",
      "  500 |   0.025846 |     0.3624 |     0.9064 |     0.8661\n",
      "  550 |   0.024085 |     0.3587 |     0.9069 |     0.8746\n",
      "  600 |   0.022948 |     0.3544 |     0.9076 |     0.8807\n",
      "  650 |   0.022085 |     0.3497 |     0.9086 |     0.8852\n",
      "  700 |   0.021348 |     0.3449 |     0.9097 |     0.8887\n",
      "  750 |   0.020676 |     0.3400 |     0.9109 |     0.8916\n",
      "  800 |   0.020042 |     0.3350 |     0.9121 |     0.8940\n",
      "  850 |   0.019434 |     0.3301 |     0.9133 |     0.8960\n",
      "  900 |   0.018848 |     0.3252 |     0.9146 |     0.8979\n",
      "  950 |   0.018280 |     0.3203 |     0.9158 |     0.8997\n",
      " 1000 |   0.017730 |     0.3155 |     0.9171 |     0.9013\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.], # bias(b)\n",
    "    [1., 0., 3., 0., 5.], \n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1, 3), -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) # b가 없다\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivo2V3xK11xx"
   },
   "source": [
    "# Custom Gradient\n",
    "* tf.train.GradientDescentOptimizer(): optimizer\n",
    "* optimizer.apply_gradients(): update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6779,
     "status": "ok",
     "timestamp": 1549859061465,
     "user": {
      "displayName": "Sean-June Lee",
      "photoUrl": "https://lh3.googleusercontent.com/-j96Pvw9LbEw/AAAAAAAAAAI/AAAAAAACQxw/aY8i4PoItwc/s64/photo.jpg",
      "userId": "12576390002043678893"
     },
     "user_tz": -540
    },
    "id": "QkPqZbkDiqe7",
    "outputId": "89754887-b544-4908-9e2d-2d9ab0dabc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |  21.726822\n",
      "   50 |   0.258689\n",
      "  100 |   0.176868\n",
      "  150 |   0.120926\n",
      "  200 |   0.082678\n",
      "  250 |   0.056528\n",
      "  300 |   0.038649\n",
      "  350 |   0.026424\n",
      "  400 |   0.018067\n",
      "  450 |   0.012352\n",
      "  500 |   0.008445\n",
      "  550 |   0.005774\n",
      "  600 |   0.003948\n",
      "  650 |   0.002699\n",
      "  700 |   0.001845\n",
      "  750 |   0.001262\n",
      "  800 |   0.000863\n",
      "  850 |   0.000590\n",
      "  900 |   0.000403\n",
      "  950 |   0.000276\n",
      " 1000 |   0.000189\n"
     ]
    }
   ],
   "source": [
    "# Multi-variable linear regression (1)\n",
    "\n",
    "X = tf.constant([[1., 2.], \n",
    "                 [3., 4.]])\n",
    "y = tf.constant([[1.5], [3.5]])\n",
    "\n",
    "W = tf.Variable(tf.random.normal((2, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "n_epoch = 1000+1\n",
    "print(\"epoch | cost\")\n",
    "for i in range(n_epoch):\n",
    "    # Use tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, W) + b\n",
    "        cost = tf.reduce_mean(tf.square(y_pred - y))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    grads = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # updates parameters (W and b)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
    "    if i % 50 == 0:\n",
    "        print(\"{:5} | {:10.6f}\".format(i, cost.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGyEI57HLRWl"
   },
   "source": [
    "# Predicting exam score\n",
    "regression using three inputs (x1, x2, x3)\n",
    "\n",
    "x1 (quiz 1) | x2 (quiz 2) | x3 (mid 1) | Y (final)\n",
    "---- | ---- | ----| ----\n",
    "73 | 80 | 75 | 152\n",
    "93 | 88 | 93 | 185\n",
    "89 | 91 | 90 | 180\n",
    "96 | 98 | 100 | 196\n",
    "73 | 66 | 70 | 142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ny-s4htZ4Lzo"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F1bSir9448BC"
   },
   "source": [
    "\n",
    "```python\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(10.)\n",
    "w2 = tf.Variable(10.)\n",
    "w3 = tf.Variable(10.)\n",
    "b  = tf.Variable(10.)\n",
    "\n",
    "hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2003,
     "status": "ok",
     "timestamp": 1549861013021,
     "user": {
      "displayName": "Sean-June Lee",
      "photoUrl": "https://lh3.googleusercontent.com/-j96Pvw9LbEw/AAAAAAAAAAI/AAAAAAACQxw/aY8i4PoItwc/s64/photo.jpg",
      "userId": "12576390002043678893"
     },
     "user_tz": -540
    },
    "id": "ICTAblBh-3FO",
    "outputId": "f364e89a-1df9-4635-a1ae-7a4b10a56659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 5793889.5000\n",
      "   50 |   64291.1562\n",
      "  100 |     715.2903\n",
      "  150 |       9.8461\n",
      "  200 |       2.0152\n",
      "  250 |       1.9252\n",
      "  300 |       1.9210\n",
      "  350 |       1.9177\n",
      "  400 |       1.9145\n",
      "  450 |       1.9114\n",
      "  500 |       1.9081\n",
      "  550 |       1.9050\n",
      "  600 |       1.9018\n",
      "  650 |       1.8986\n",
      "  700 |       1.8955\n",
      "  750 |       1.8923\n",
      "  800 |       1.8892\n",
      "  850 |       1.8861\n",
      "  900 |       1.8829\n",
      "  950 |       1.8798\n",
      " 1000 |       1.8767\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(10.)\n",
    "w2 = tf.Variable(10.)\n",
    "w3 = tf.Variable(10.)\n",
    "b  = tf.Variable(10.)\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1,w2,w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ptg1sl3w8uFt"
   },
   "source": [
    "## Multi-variable linear regression (1)\n",
    "*  random  초기화: tf.random_normal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1992,
     "status": "ok",
     "timestamp": 1549861572665,
     "user": {
      "displayName": "Sean-June Lee",
      "photoUrl": "https://lh3.googleusercontent.com/-j96Pvw9LbEw/AAAAAAAAAAI/AAAAAAACQxw/aY8i4PoItwc/s64/photo.jpg",
      "userId": "12576390002043678893"
     },
     "user_tz": -540
    },
    "id": "LhczGchq_zzQ",
    "outputId": "37949eab-e23e-4ef3-997a-e0e02f63daa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   29366.5117\n",
      "   50 |     349.3437\n",
      "  100 |      27.3175\n",
      "  150 |      23.6930\n",
      "  200 |      23.6017\n",
      "  250 |      23.5498\n",
      "  300 |      23.4984\n",
      "  350 |      23.4471\n",
      "  400 |      23.3960\n",
      "  450 |      23.3451\n",
      "  500 |      23.2943\n",
      "  550 |      23.2437\n",
      "  600 |      23.1931\n",
      "  650 |      23.1428\n",
      "  700 |      23.0925\n",
      "  750 |      23.0422\n",
      "  800 |      22.9923\n",
      "  850 |      22.9424\n",
      "  900 |      22.8928\n",
      "  950 |      22.8430\n",
      " 1000 |      22.7937\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# random weights\n",
    "w1 = tf.Variable(tf.random.normal((1,)))\n",
    "w2 = tf.Variable(tf.random.normal((1,)))\n",
    "w3 = tf.Variable(tf.random.normal((1,)))\n",
    "b  = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1,w2,w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7w0kEX9C82D8"
   },
   "source": [
    "## Multi-variable linear regression (2)\n",
    "\n",
    "* Matrix 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2082,
     "status": "ok",
     "timestamp": 1549861764291,
     "user": {
      "displayName": "Sean-June Lee",
      "photoUrl": "https://lh3.googleusercontent.com/-j96Pvw9LbEw/AAAAAAAAAAI/AAAAAAACQxw/aY8i4PoItwc/s64/photo.jpg",
      "userId": "12576390002043678893"
     },
     "user_tz": -540
    },
    "id": "mmNogkEuQe5K",
    "outputId": "025477a5-868f-413d-fb4c-678f9510a6be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 | 201300.4688\n",
      "  100 |    74.4386\n",
      "  200 |    49.4028\n",
      "  300 |    49.1406\n",
      "  400 |    48.8831\n",
      "  500 |    48.6269\n",
      "  600 |    48.3722\n",
      "  700 |    48.1188\n",
      "  800 |    47.8666\n",
      "  900 |    47.6159\n",
      " 1000 |    47.3666\n",
      " 1100 |    47.1186\n",
      " 1200 |    46.8720\n",
      " 1300 |    46.6266\n",
      " 1400 |    46.3825\n",
      " 1500 |    46.1397\n",
      " 1600 |    45.8983\n",
      " 1700 |    45.6582\n",
      " 1800 |    45.4195\n",
      " 1900 |    45.1820\n",
      " 2000 |    44.9457\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "\n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mt9zvM2OaljB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5288262],\n",
       "       [-1.1683029],\n",
       "       [ 1.6296498]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCTzJDjH2k4z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66334724], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cs-KBzAHCf2u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[141.02716],\n",
       "       [191.59097],\n",
       "       [177.08179],\n",
       "       [195.90196],\n",
       "       [149.23515]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FgoGhG8GxEW"
   },
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZcVkvquG0g5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152.0, 185.0, 180.0, 196.0, 142.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y # labels, 실제값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_YW4xsrGZAE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[141.02716],\n",
       "       [191.59097],\n",
       "       [177.08179],\n",
       "       [195.90196],\n",
       "       [149.23515]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X).numpy() # prediction, 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrXNF_NSHZGo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[175.6679 ],\n",
       "       [160.12111]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 데이터에 대한 예측\n",
    "\n",
    "predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05 Logistic Classification(Regression) - Eager Execution\n",
    "* Logistic Classfication은 True or False와 같은 Binary나 복수개의 다항 분류에 쓰입니다 (Bernoulli Distribution)\n",
    "\n",
    "### 기본 Library 선언 및 Tensorflow 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(777)  # for reproducibility\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강의에 설명할 Data입니다\n",
    "* x_data가 2차원 배열이기에 2차원 공간에 표현하여 x1과 x2를 기준으로 y_data 0과 1로 구분하는 예제입니다\n",
    "* Logistic Classification 통해 보라색과 노란색 y_data(Label)을 구분해 보겠습니다.\n",
    "* Test 데이터는 붉은색의 위치와 같이 추론시 1의 값을 가지게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = [[1., 2.],\n",
    "          [2., 3.],\n",
    "          [3., 1.],\n",
    "          [4., 3.],\n",
    "          [5., 3.],\n",
    "          [6., 2.]]\n",
    "y_train = [[0.],\n",
    "          [0.],\n",
    "          [0.],\n",
    "          [1.],\n",
    "          [1.],\n",
    "          [1.]]\n",
    "\n",
    "x_test = [[5.,2.]]\n",
    "y_test = [[1.]]\n",
    "\n",
    "\n",
    "x1 = [x[0] for x in x_train]\n",
    "x2 = [x[1] for x in x_train]\n",
    "\n",
    "colors = [int(y[0] % 3) for y in y_train]\n",
    "plt.scatter(x1,x2, c=colors , marker='^')\n",
    "plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tensorflow Eager\n",
    "### 위 Data를 기준으로 가설의 검증을 통해 Logistic Classification 모델을 만들도록 하겠습니다\n",
    "* Tensorflow data API를 통해 학습시킬 값들을 담는다 (Batch Size는 한번에 학습시킬 Size로 정한다)\n",
    "* features,labels는 실재 학습에 쓰일 Data (연산을 위해 Type를 맞춰준다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))#.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 Data를 기준으로 가설의 검증을 통해 Logistic Classification 모델을 만들도록 하겠습니다\n",
    "* W와 b은 학습을 통해 생성되는 모델에 쓰이는 Wegith와 Bias (초기값을 variable : 0이나 Random값으로 가능 tf.random_normal([2, 1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([2,1]), name='weight')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid 함수를 가설로 선언합니다\n",
    "* Sigmoid는 아래 그래프와 같이 0과 1의 값만을 리턴합니다 tf.sigmoid(tf.matmul(X, W) + b)와 같습니다\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "sigmoid(x) & = \\frac{1}{1+e^{-x}}  \\\\\\\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features):\n",
    "    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))\n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설을 검증할 Cost 함수를 정의합니다\n",
    "$$\n",
    "\\begin{align}\n",
    "cost(h(x),y) & = −log(h(x))  &  if  &  y=1 \\\\\\\\\\\n",
    "cost(h(x),y) & = -log(1−h(x))  &  if  &  y=0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위 두수식을 합치면 아래과 같습니다\n",
    "$$\n",
    "\\begin{align}\n",
    "cost(h(x),y) & = −y log(h(x))−(1−y)log(1−h(x))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(hypothesis, features, labels):\n",
    "    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론한 값은 0.5를 기준(Sigmoid 그래프 참조)로 0과 1의 값을 리턴합니다.\n",
    "* Sigmoid 함수를 통해 예측값이 0.5보다 크면 1을 반환하고 0.5보다 작으면 0으로 반환합니다.\n",
    "* 가설을 통해 실재 값과 비교한 정확도를 측정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientTape를 통해 경사값을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(features, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_fn(logistic_regression(features),features,labels)\n",
    "    return tape.gradient(loss_value, [W,b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 실행합니다.\n",
    "* 위의 Data를 Cost함수를 통해 학습시킨 후 모델을 생성합니다. \n",
    "* 새로운 Data를 통한 검증 수행 [5,2]의 Data로 테스트 수행 (그래프상 1이 나와야 정상입니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6874\n",
      "Iter: 100, Loss: 0.5776\n",
      "Iter: 200, Loss: 0.5349\n",
      "Iter: 300, Loss: 0.5054\n",
      "Iter: 400, Loss: 0.4838\n",
      "Iter: 500, Loss: 0.4671\n",
      "Iter: 600, Loss: 0.4535\n",
      "Iter: 700, Loss: 0.4420\n",
      "Iter: 800, Loss: 0.4319\n",
      "Iter: 900, Loss: 0.4228\n",
      "Iter: 1000, Loss: 0.4144\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1001\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "    for features, labels  in iter(dataset):\n",
    "        grads = grad(features, labels)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
    "        if step % 100 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(logistic_regression(features),features,labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab-04-1-Multi-variable-Linear-Regression-eager.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
